Title,Authors,Publication date,Journal,Volume,Issue,Publisher,Description,Total citations,Scholar articles,Citation Count,Pages,Conference,Institution
Multi-modal point-of-care diagnostics for covid-19 based on acoustics and symptoms,"Srikanth Raj Chetupalli, Prashant Krishnan, Neeraj Sharma, Ananya Muguli, Rohit Kumar, Viral Nanda, Lancelot Mark Pinto, Prasanta Kumar Ghosh, Sriram Ganapathy",2023/3/8,IEEE Journal of Translational Engineering in Health and Medicine,11.0,,IEEE,"Background:
The COVID-19 pandemic has highlighted the need to invent alternative respiratory health diagnosis methodologies which provide improvement with respect to time, cost, physical distancing and detection performance. In this context, identifying acoustic bio-markers of respiratory diseases has received renewed interest.
Objective:
In this paper, we aim to design COVID-19 diagnostics based on analyzing the acoustics and symptoms data. Towards this, the data is composed of cough, breathing, and speech signals, and health symptoms record, collected using a web-application over a period of twenty months.
Methods:
We investigate the use of time-frequency features for acoustic signals and binary features for encoding different health symptoms. We experiment with use of classifiers like logistic regression, support vector machines and long-short term memory (LSTM) network models on the acoustic data …",11,"Multi-modal point-of-care diagnostics for covid-19 based on acoustics and symptoms
SR Chetupalli, P Krishnan, N Sharma, A Muguli… - IEEE Journal of Translational Engineering in Health …, 2023
Cited by 11 Related articles All 7 versions","{'2022': 4, '2023': 3}",199-210,,
Decoding attended talker solely from listening-state EEG signals,"Mohamed Elminshawi, Julia Kostina, Emanuël AP Habets, Neeraj Kumar Sharma",2022/7/4,"Book of Abstracts, VoiceID Conference Proc.",,,,,0,"Decoding attended talker solely from listening-state EEG signals*
M Elminshawi, J Kostina, EAP Habets, NK Sharma - Book of Abstracts
Related articles All 2 versions",{},22,,
Analyzing the impact of SARS-CoV-2 variants on respiratory sound signals,"Debarpan Bhattacharya, Debottam Dutta, Neeraj Kumar Sharma, Srikanth Raj Chetupalli, Pravin Mote, Sriram Ganapathy, Sahiti Nori, Sadhana Gonuguntla, Murali Alagesan",2022/6/24,,,,,"The COVID-19 outbreak resulted in multiple waves of infections that have been associated with different SARS-CoV-2 variants. Studies have reported differential impact of the variants on respiratory health of patients. We explore whether acoustic signals, collected from COVID-19 subjects, show computationally distinguishable acoustic patterns suggesting a possibility to predict the underlying virus variant. We analyze the Coswara dataset which is collected from three subject pools, namely, i) healthy, ii) COVID-19 subjects recorded during the delta variant dominant period, and iii) data from COVID-19 subjects recorded during the omicron surge. Our findings suggest that multiple sound categories, such as cough, breathing, and speech, indicate significant acoustic feature differences when comparing COVID-19 subjects with omicron and delta variants. The classification areas-under-the-curve are significantly above chance for differentiating subjects infected by omicron from those infected by delta. Using a score fusion from multiple sound categories, we obtained an area-under-the-curve of 89% and 52.4% sensitivity at 95% specificity. Additionally, a hierarchical three class approach was used to classify the acoustic data into healthy and COVID-19 positive, and further COVID-19 subjects into delta and omicron variants providing high level of 3-class classification accuracy. These results suggest new ways for designing sound based COVID-19 diagnosis approaches.",1,"Analyzing the impact of SARS-CoV-2 variants on respiratory sound signals
D Bhattacharya, D Dutta, NK Sharma, SR Chetupalli… - arXiv preprint arXiv:2206.12309, 2022
Cited by 1 Related articles All 8 versions",{'2023': 1},,"Interspeech 2022, Incheon",
Coswara: A website application enabling COVID-19 screening by analysing respiratory sound samples and health symptoms,"Debarpan Bhattacharya, Debottam Dutta, Neeraj Kumar Sharma, Srikanth Raj Chetupalli, Pravin Mote, Sriram Ganapathy, Sahiti Nori, Sadhana Gonuguntla, Murali Alagesan",2022/6/9,,,,,"The COVID-19 pandemic has accelerated research on design of alternative, quick and effective COVID-19 diagnosis approaches. In this paper, we describe the Coswara tool, a website application designed to enable COVID-19 detection by analysing respiratory sound samples and health symptoms. A user using this service can log into a website using any device connected to the internet, provide there current health symptom information and record few sound sampled corresponding to breathing, cough, and speech. Within a minute of analysis of this information on a cloud server the website tool will output a COVID-19 probability score to the user. As the COVID-19 pandemic continues to demand massive and scalable population level testing, we hypothesize that the proposed tool provides a potential solution towards this.",0,"Coswara: A website application enabling COVID-19 screening by analysing respiratory sound samples and health symptoms
D Bhattacharya, D Dutta, NK Sharma, SR Chetupalli… - arXiv preprint arXiv:2206.05053, 2022
Related articles All 5 versions",{},,"Show and Tell, Interspeech 2022, Incheon",
The Second Dicova Challenge: Dataset and Performance Analysis for Diagnosis of Covid-19 Using Acoustics,"Neeraj Kumar Sharma, Srikanth Raj Chetupalli, Debarpan Bhattacharya, Debottam Dutta, Pravin Mote, Sriram Ganapathy",2022/5/23,,,,IEEE,"The Second Diagnosis of COVID-19 using Acoustics (DiCOVA) Challenge aimed at accelerating the research in acoustics based detection of COVID-19, a topic at the intersection of acoustics, signal processing, machine learning, and healthcare. This paper presents the details of the challenge, which was an open call for researchers to analyze a dataset of audio recordings consisting of breathing, cough and speech signals. This data was collected from individuals with and without COVID-19 infection, and the task in the challenge was a two-class classification. The development set audio recordings were collected from 965 (172 COVID-19 positive) individuals, while the evaluation set contained data from 471 individuals (71 COVID-19 positive). The challenge featured four tracks, one associated with each sound category of cough, speech and breathing, and a fourth fusion track. A baseline system was also …",25,"The Second DiCOVA Challenge: Dataset and performance analysis for COVID-19 diagnosis using acoustics*
NK Sharma, SR Chetupalli, D Bhattacharya, D Dutta… - arXiv preprint arXiv:2110.01177, 2021
Cited by 19 Related articles All 8 versions
The Second Dicova Challenge: Dataset and Performance Analysis for Diagnosis of Covid-19 Using Acoustics
NK Sharma, SR Chetupalli, D Bhattacharya, D Dutta… - ICASSP 2022-2022 IEEE International Conference on …, 2022
Cited by 8 Related articles All 7 versions","{'2021': 1, '2022': 19, '2023': 5}",556-560,"ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
Towards sound based testing of COVID-19--Summary of the first Diagnostics of COVID-19 using Acoustics (DiCOVA) Challenge,"Neeraj Kumar Sharma, Ananya Muguli, Prashant Krishnan, Rohit Kumar, Srikanth Raj Chetupalli, Sriram Ganapathy",2021/6/21,Elsevier Computer Speech and Language,,,,"The technology development for point-of-care tests (POCTs) targeting respiratory diseases has witnessed a growing demand in the recent past. Investigating the presence of acoustic biomarkers in modalities such as cough, breathing and speech sounds, and using them for building POCTs can offer fast, contactless and inexpensive testing. In view of this, over the past year, we launched the “Coswara” project to collect cough, breathing and speech sound recordings via worldwide crowdsourcing. With this data, a call for development of diagnostic tools was announced in the Interspeech 2021 as a special session titled “Diagnostics of COVID-19 using Acoustics (DiCOVA) Challenge”. The goal was to bring together researchers and practitioners interested in developing acoustics-based COVID-19 POCTs by enabling them to work on the same set of development and test datasets. As part of the challenge, datasets …",12,"Towards sound based testing of COVID-19—Summary of the first Diagnostics of COVID-19 using Acoustics (DiCOVA) Challenge
NK Sharma, A Muguli, P Krishnan, R Kumar… - Computer Speech & Language, 2022
Cited by 12 Related articles All 13 versions","{'2021': 2, '2022': 7, '2023': 2}",,,
"DiCOVA Challenge: Dataset, task, and baseline system for COVID-19 diagnosis using acoustics","Ananya Muguli, Lancelot Pinto, Neeraj Sharma, Prashant Krishnan, Prasanta Kumar Ghosh, Rohit Kumar, Shrirama Bhat, Srikanth Raj Chetupalli, Sriram Ganapathy, Shreyas Ramoji, Viral Nanda",2021/3/16,,,,,"The DiCOVA challenge aims at accelerating research in diagnosing COVID-19 using acoustics (DiCOVA), a topic at the intersection of speech and audio processing, respiratory health diagnosis, and machine learning. This challenge is an open call for researchers to analyze a dataset of sound recordings collected from COVID-19 infected and non-COVID-19 individuals for a two-class classification. These recordings were collected via crowdsourcing from multiple countries, through a website application. The challenge features two tracks, one focusing on cough sounds, and the other on using a collection of breath, sustained vowel phonation, and number counting speech recordings. In this paper, we introduce the challenge and provide a detailed description of the task, and present a baseline system for the task.",81,"DiCOVA Challenge: Dataset, task, and baseline system for COVID-19 diagnosis using acoustics
A Muguli, L Pinto, N Sharma, P Krishnan, PK Ghosh… - arXiv preprint arXiv:2103.09148, 2021
Cited by 81 Related articles All 8 versions","{'2021': 31, '2022': 42, '2023': 8}",,Proc. Interspeech 2021,
Acoustic and linguistic features influence talker change detection,"Neeraj Kumar Sharma, Venkat Krishnamohan, Sriram Ganapathy, Ahana Gangopadhayay, Lauren Fink",2020/11/25,The Journal of the Acoustical Society of America,148.0,5.0,Acoustical Society of America,"A listening test is proposed in which human participants detect talker changes in two natural, multi-talker speech stimuli sets—a familiar language (English) and an unfamiliar language (Chinese). Miss rate, false-alarm rate, and response times (RT) showed a significant dependence on language familiarity. Linear regression modeling of RTs using diverse acoustic features derived from the stimuli showed recruitment of a pool of acoustic features for the talker change detection task. Further, benchmarking the same task against the state-of-the-art machine diarization system showed that the machine system achieves human parity for the familiar language but not for the unfamiliar language.",2,"Acoustic and linguistic features influence talker change detection
NK Sharma, V Krishnamohan, S Ganapathy… - The Journal of the Acoustical Society of America, 2020
Cited by 2 Related articles All 9 versions",{'2021': 2},EL414-EL419,,
"Coswara--A Database of Breathing, Cough, and Voice Sounds for COVID-19 Diagnosis","Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj Chetupalli, R Nirmala, Prasanta Kumar Ghosh, Sriram Ganapathy",2020/5/21,,,,,"The COVID-19 pandemic presents global challenges transcending boundaries of country, race, religion, and economy. The current gold standard method for COVID-19 detection is the reverse transcription polymerase chain reaction (RT-PCR) testing. However, this method is expensive, time-consuming, and violates social distancing. Also, as the pandemic is expected to stay for a while, there is a need for an alternate diagnosis tool which overcomes these limitations, and is deployable at a large scale. The prominent symptoms of COVID-19 include cough and breathing difficulties. We foresee that respiratory sounds, when analyzed using machine learning techniques, can provide useful insights, enabling the design of a diagnostic tool. Towards this, the paper presents an early effort in creating (and analyzing) a database, called Coswara, of respiratory sounds, namely, cough, breath, and voice. The sound samples are collected via worldwide crowdsourcing using a website application. The curated dataset is released as open access. As the pandemic is evolving, the data collection and analysis is a work in progress. We believe that insights from analysis of Coswara can be effective in enabling sound based technology solutions for point-of-care diagnosis of respiratory infection, and in the near future this can help to diagnose COVID-19.",271,"Coswara--a database of breathing, cough, and voice sounds for COVID-19 diagnosis
N Sharma, P Krishnan, R Kumar, S Ramoji… - arXiv preprint arXiv:2005.10548, 2020
Cited by 230 Related articles All 13 versions
NR, PK Ghosh, and S*
N Sharma, P Krishnan, R Kumar, S Ramoji… - Ganapathy,“Coswara–a database of breathing, cough …, 2020
Cited by 37 Related articles
Nirmala R*
N Sharma, P Krishnan, R Kumar, S Ramoji… - Prasanta Kumar Ghosh, and Sriram Ganapathy,“ …, 2020
Cited by 23 Related articles
Coswara—A Database of Breathing, Cough, and Voice Sounds for COVID-19 Diagnosis. arXiv 2020*
N Sharma, P Krishnan, R Kumar, S Ramoji… - arXiv preprint arXiv:2005.10548
Cited by 19 Related articles
Coswara–A Database of Breathing*
N Sharma, P Krishnan, R Kumar, S Ramoji… - Cough, and Voice Sounds for COVID-19 Diagnosis …, 2020
Cited by 11 Related articles","{'2020': 8, '2021': 106, '2022': 128, '2023': 26}",4811--4815,Proc. Interspeech 2020,
On the impact of language familiarity in talker change detection,"Neeraj Sharma, Venkat Krishnamohan, Sriram Ganapathy, Ahana Gangopadhayay, Lauren Fink",2020/5/4,,,,IEEE,"The ability to detect talker changes when listening to conversational speech is fundamental to perception and understanding of multi-talker speech. In this paper, we propose an experimental paradigm to provide insights on the impact of language familiarity on talker change detection. Two multi-talker speech stimulus sets, one in a language familiar to the listeners (English) and the other unfamiliar (Chinese), are created. A listening test is performed in which listeners indicate the number of talkers in the presented stimuli. Analysis of human performance shows statistically significant results for: (a) lower miss (and a higher false alarm) rate in familiar versus unfamiliar language, and (b) longer response time in familiar versus unfamiliar language. These results signify a link between perception of talker attributes and language proficiency. Subsequently, a machine system is designed to perform the same task. The …",1,"On the impact of language familiarity in talker change detection
N Sharma, V Krishnamohan, S Ganapathy… - ICASSP 2020-2020 IEEE International Conference on …, 2020
Cited by 1 Related articles All 6 versions",{'2020': 1},6249-6253,"ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
Analyzing Human Reaction Time for Talker Change Detection,"Neeraj Sharma, Shobhana Ganesh, Sriram Ganapathy, Lori L Holt",2019/5/12,,,,IEEE,"The ability to detect a change in the input is an essential aspect of perception. In speech communication, we use this ability to identify ""talker changes"" when listening to conversational speech (such as, audio podcasts). In this paper, we propose to improve our understanding about how fast listeners detect a change in talker, and the acoustic features tracked to identify a voice by designing a novel experimental paradigm. A listening experiment is designed in which listeners indicate the moment of perceived talker change in multi-talker speech utterances. We examine talker change detection performance by probing the human reaction time (RT). A random forest regression is used to model the relationship between RTs and acoustic features. The findings suggest that: (i) RT is less than a second, (ii) RT can be predicted from the difference in acoustic features of segment before and after change, and (iii) there a exists …",1,"Analyzing human reaction time for talker change detection
N Sharma, S Ganesh, S Ganapathy, LL Holt - ICASSP 2019-2019 IEEE International Conference on …, 2019
Cited by 1 Related articles All 3 versions",{'2020': 1},7135-7139,"ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
Talker change detection: A comparison of human and machine performance,"Neeraj Kumar Sharma, Shobhana Ganesh, Sriram Ganapathy, Lori L Holt",2019/1/7,The Journal of the Acoustical Society of America,145.0,1.0,ASA,"The automatic analysis of conversational audio remains difficult, in part, due to the presence of multiple talkers speaking in turns, often with significant intonation variations and overlapping speech. The majority of prior work on psychoacoustic speech analysis and system design has focused on single-talker speech or multi-talker speech with overlapping talkers (for example, the cocktail party effect). There has been much less focus on how listeners detect a change in talker or in probing the acoustic features significant in characterizing a talker's voice in conversational speech. This study examines human talker change detection (TCD) in multi-party speech utterances using a behavioral paradigm in which listeners indicate the moment of perceived talker change. Human reaction times in this task can be well-estimated by a model of the acoustic feature distance among speech segments before and after a change in …",8,"Talker change detection: A comparison of human and machine performance
NK Sharma, S Ganesh, S Ganapathy, LL Holt - The Journal of the Acoustical Society of America, 2019
Cited by 8 Related articles All 7 versions","{'2019': 2, '2020': 2, '2021': 2, '2022': 2}",131-142,,
Time-varying sinusoidal demodulation for non-stationary modeling of speech,"Neeraj Kumar Sharma, Thippur V Sreenivas",2018/12/1,Speech Communication,105.0,,North-Holland,"Speech signals contain a fairly rich time-evolving spectral content. Accurate analysis of this time-evolving spectrum is an open challenge in signal processing. Towards this, we visit time-varying sinusoidal modeling of speech and propose an alternate model estimation approach. The estimation operates on the whole signal without any short-time analysis. The approach proceeds by extracting the fundamental frequency sinusoid (FFS) from speech signal. The instantaneous amplitude (IA) of the FFS is used for voiced/unvoiced stream segregation. The voiced stream is then demodulated using a variant of in-phase and quadrature-phase demodulation carried at harmonics of the FFS. The result is a non-parametric time-varying sinusoidal representation, specifically, an additive mixture of quasi-harmonic sinusoids for voiced stream and a wideband mono-component sinusoid for unvoiced stream. The representation is …",1,"Time-varying sinusoidal demodulation for non-stationary modeling of speech
NK Sharma, TV Sreenivas - Speech Communication, 2018
Cited by 1 Related articles All 3 versions",{'2020': 1},77-91,,
Information-rich sampling of time-varying signals,Neeraj Kumar Sharma,2018/6/5,,,,,"Confrontation with signal non-stationarity is a rule rather than an exception in the analysis of natural signals, such as speech, animal vocalization, music, bio-medical, atmospheric, ans seismic signals. Interestingly, our auditory system analyzes signal non-stationarity to trigger our perception. It does this with a performance which is unparalleled when compared to any man-made sound analyzer. Non-stationary signal analysis is a fairly challenging problem in the expanse of signal processing. Conventional approaches to analyze non-stationary signals are based on short-time quasi- stationary assumptions. Typically, short-time signal segments are analyzed using one of several transforms, such as Fourier, chirplets, and wavelets, with a predefined basis. However, the quasi-stationary assumption is known to be a serious limitation in recognizing fine temporal and spectral variations in natural signals. An accurate …",0,"Information-rich sampling of time-varying signals
NK Sharma - 2018",{},,,Indian Institute of Science
Leveraging LSTM models for overlap detection in multi-party meetings,"Neeraj Sajjan, Shobhana Ganesh, Neeraj Sharma, Sriram Ganapathy, Neville Ryant",2018/4/15,,,,IEEE,"The detection of overlapping speech segments is of key importance in speech applications involving analysis of multi-party conversations. The detection problem is challenging because overlapping speech segments are typically captured as short speech utterances far-field microphone recordings. In this paper, we propose detection of overlap segments using a neural network architecture consisting of long-short term memory (LSTM) models. The neural network architecture learns the presence of overlap in speech by identifying the spectrotemporal structure of overlapping speech segments. In order to evaluate the model performance, we perform experiments on simulated overlapped speech generated from the TIMIT database, and natural multi-talker conversational speech in the augmented Multiparty Interaction (AMI) meeting corpus. The proposed approach yields improvements over a Gaussian mixture model …",20,"Leveraging LSTM models for overlap detection in multi-party meetings
N Sajjan, S Ganesh, N Sharma, S Ganapathy, N Ryant - 2018 IEEE International Conference on Acoustics …, 2018
Cited by 20 Related articles All 8 versions","{'2018': 1, '2019': 2, '2020': 4, '2021': 6, '2022': 6, '2023': 1}",5249-5253,"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
Multicomponent 2-D AM-FM Modeling of Speech Spectrograms.,"Jitendra Kumar Dhiman, Neeraj Sharma, Chandra Sekhar Seelamantula",2018,,,,,"In contrast to 1-D short-time analysis of speech, 2-D modeling of spectrograms provides a characterization of speech attributes directly in the joint time-frequency plane. Building on existing 2-D models to analyze a spectrogram patch, we propose a multicomponent 2-D AM-FM representation for spectrogram decomposition. The components of the proposed representation comprise a DC, a fundamental frequency carrier and its harmonics, and a spectrotemporal envelope, all in 2-D. The number of harmonics required is patch-dependent. The estimation of the AM and FM is done using the Riesz transform, and the component weights are estimated using a least-squares approach. The proposed representation provides an improvement over existing state-of-the-art approaches, for both male and female speakers. This is quantified using reconstruction SNR and perceptual evaluation of speech quality (PESQ) metric …",0,"Multicomponent 2-D AM-FM Modeling of Speech Spectrograms.
JK Dhiman, N Sharma, CS Seelamantula - INTERSPEECH, 2018
Related articles All 5 versions",{},736-740,Interspeech,
Dimension-based Attention in Learning and Understanding Spoken Language,"Frederic K Dick, Lori L Holt, Howard Nusbaum, Neeraj Sharma, Barbara Shinn-Cunningham",2018,Sciences,114.0,,,"Background
The acoustic world is variable and messy. Signals that allow listeners to identify and/or disambiguate important auditory objects (an unusual rustle of leaves in a darkening forest, your winning bingo number) will likely share many acoustic characteristics with other sounds in the environment. What is more, the most diagnostic auditory dimensions for detecting the ominously rustling leaves or categorizing a crucial syllable may be obscured, distorted, or missing. Just as often, the diagnostic acoustic dimensions may change completely across contexts.
These challenges are ubiquitous in speech comprehension. People talk with different accents, different speaking rates, different vocal tracts, and often at the same time. A talker may turn away from you in the middle of a conversation, and then decide to whisper the most intriguing part of their sentence, so as not to be overheard by the person they are talking about. Or, worse yet, the talker may imitate the very different and distinct voice of the object of conversation. Each of these cases dramatically changes the character, usefulness, and significance of the acoustic dimensions available for speech comprehension. This variability and context-dependence in the mapping between acoustic dimensions and the identity or significance of a sound segment means that an'acoustical fingerprint'computational approach that has been so successful in recognizing recorded music even in degraded recordings and noisy backgrounds (Wang, 2003) may not be very successful for humans or machines trying to perceive and understand natural speech in everyday environments.",0,"Dimension-based Attention in Learning and Understanding Spoken Language
FK Dick, LL Holt, H Nusbaum, N Sharma… - Sciences, 2018
All 3 versions",{},6352-6537,,
LEAP Submission for DIHARD 2018,"Shobhana Ganesh, P Bharat, Neeraj Sharma, Prachi Singh, Sriram Ganapathy",,,,,,"The problem of associating segments in an audio signal with a particular speaker to answer the question of ‘who spoke when’, also referred to as speaker diarization, has gained considerable interest owing to its significance as a pre-processing step in automatic speech recognition applications. While diarizing systems perform well on clean datasets such as telephone conversations and interviews, the performance on datasets associated with meetings, child speech, multiple number of speakers with short turns in conversations, etc., remains a byzantine task. In this report, we describe our system designed for diarization of data drawn from the later kinds of datasets. This system was submitted to the First DIHARD Speech Diarization Challenge, 2018. The system makes use of MFCCs as front-end features, followed by an i-vector modeling and subsequently, PLDA scoring followed by agglomerative clustering. This set up uses ivectors, obtained from a GMM-UBM to detect change in speakers and an unsupervised calibration method to estimate the number of speakers. We obtain a DER of 28.52 (and MI of 8.32) and 53.4 (and MI of 7.6) on the evaluation data in track 1 and track 2, respectively.",0,"LEAP Submission for DIHARD 2018
S Ganesh, P Bharat, N Sharma, P Singh, S Ganapathy
Related articles",{},,,
